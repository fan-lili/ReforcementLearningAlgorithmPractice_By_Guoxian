{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0d74d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.2 (SDL 2.0.18, Python 3.9.9)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "# 探索初始蒙特卡洛算法\n",
    "import random\n",
    "import time\n",
    "from yuanyangEnv import YuanYangEnv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2700402",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MC_RL:\n",
    "    def __init__(self,yuanyang):\n",
    "        # 行为值函数的初始化\n",
    "        self.qvalue = np.ones((len(yuanyang.states),len(yuanyang.actions)))*0.1\n",
    "        \n",
    "        # 次数初始化，求经验平均时，q(s,a)=G(s,a)/n(s,a)\n",
    "        self.n = 0.001*np.ones((len(yuanyang.states),len(yuanyang.actions)))\n",
    "        self.actions = yuanyang.actions\n",
    "        self.gamma = yuanyang.gamma\n",
    "        self.yuanyang = yuanyang\n",
    "\n",
    "    def greedy_policy(self,qfun,state):\n",
    "        # 贪婪策略\n",
    "        amax = qfun[state,:].argmax()\n",
    "        return self.actions[amax]\n",
    "\n",
    "    def epsilon_greedy_policy(self,qfun,state,epsilon):\n",
    "        # ε-greedy策略\n",
    "        amax = qfun[state,:].argmax()\n",
    "        if np.random.uniform() < 1 - epsilon:\n",
    "            return self.greedy_policy(qfun,state)\n",
    "        else:\n",
    "            return self.actions[int(np.random.rand()*len(self.actions))]\n",
    "\n",
    "    def find_anum(self,a):\n",
    "        # 找到动作对应的序号\n",
    "        for i in range(len(self.actions)):\n",
    "            if a == self.actions[i]:\n",
    "                return i\n",
    "\n",
    "    def mc_learning_ei(self, num_iter):\n",
    "        # 学习num_iter次\n",
    "        self.qvalue = np.zeros((len(yuanyang.states), len(yuanyang.actions)))\n",
    "        self.n = 0.001 * np.ones((len(yuanyang.states), len(yuanyang.actions)))\n",
    "        for iter in range(num_iter):\n",
    "            # 采集状态样本\n",
    "            s_sample = []\n",
    "            \n",
    "            # 采集动作样本\n",
    "            a_sample = []\n",
    "            \n",
    "            # 采集回报样本\n",
    "            r_sample = []\n",
    "            \n",
    "            # 随机初始化状态\n",
    "            s = self.yuanyang.reset()\n",
    "            a = self.actions[int(random.random() * len(self.actions))]\n",
    "            done = False\n",
    "            step_num = 0\n",
    "            if self.mc_test() == 1:\n",
    "                print(\"探索初始化第一次完成任务需要的次数：\", iter)\n",
    "                break\n",
    "                \n",
    "            # 采集数据s0-a1-s1-a2-s2...terminate state\n",
    "            # for i in range(5):\n",
    "            while False == done and step_num < 30:\n",
    "                # 与环境交互\n",
    "                s_next, r, done = self.yuanyang.transform(s, a)\n",
    "                a_num = self.find_anum(a)\n",
    "                \n",
    "                # 往回走给予惩罚\n",
    "                if s_next in s_sample:\n",
    "                    r = -2\n",
    "                    \n",
    "                # 存储数据，采样数据\n",
    "                s_sample.append(s)\n",
    "                r_sample.append(r)\n",
    "                a_sample.append(a_num)\n",
    "                step_num += 1\n",
    "                \n",
    "                # 转移到下一个状态，继续试验，s0-s1-s2\n",
    "                s = s_next\n",
    "                a = self.greedy_policy(self.qvalue, s)\n",
    "                \n",
    "            # 从样本中计算累计回报,g(s_0) = r_0+gamma*r_1+gamma^2*r_2+gamma^3*r3+v(sT)\n",
    "            a = self.greedy_policy(self.qvalue, s)\n",
    "            g = self.qvalue[s, self.find_anum(a)]\n",
    "            \n",
    "            # 计算该序列的第一状态的累计回报\n",
    "            for i in range(len(s_sample) - 1, -1, -1):\n",
    "                g *= self.gamma\n",
    "                g += r_sample[i]\n",
    "                \n",
    "            # print(\"episode number, trajectory\", iter1, s_sample)\n",
    "            # print(\"first state\", s_sample[0], g)\n",
    "            # g=G(s1,a),开始算其他状态处的累计回报\n",
    "            for i in range(len(s_sample)):\n",
    "                # 计算状态-行为对（s,a)的次数，s,a1...s,a2\n",
    "                self.n[s_sample[i], a_sample[i]] += 1.0\n",
    "                \n",
    "                # 利用增量式方法更新值函数\n",
    "                self.qvalue[s_sample[i], a_sample[i]] = (self.qvalue[s_sample[i], a_sample[i]] * (\n",
    "                        self.n[s_sample[i], a_sample[i]] - 1) + g) / self.n[s_sample[i], a_sample[i]]\n",
    "                \n",
    "                g -= r_sample[i]\n",
    "                g /= self.gamma\n",
    "                # print(\"s_sample,a\",g)\n",
    "            # print(\"number\",self.n)\n",
    "        return self.qvalue\n",
    "\n",
    "    def mc_test(self):\n",
    "        # 初始状态为0，智能体通过贪婪策略与环境进行交互，如果结束时，智能体找到了目标，则设置标志位为1.否则为0\n",
    "        s = 0\n",
    "        s_sample = []\n",
    "        done = False\n",
    "        flag = 0\n",
    "        step_num = 0\n",
    "        while False == done and step_num < 30:\n",
    "            a = self.greedy_policy(self.qvalue, s)\n",
    "            \n",
    "            # 与环境交互\n",
    "            s_next, r, done = self.yuanyang.transform(s, a)\n",
    "            s_sample.append(s)\n",
    "            s = s_next\n",
    "            step_num += 1\n",
    "        if s == 9:\n",
    "            flag = 1    # 到达目标\n",
    "        return flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e848715e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "探索初始化第一次完成任务需要的次数： 1663\n",
      "[[-28.51974222 -27.1942155  -27.47661624 -28.25840333]\n",
      " [-27.09702259 -27.38328463 -28.45194141 -28.05795824]\n",
      " [-26.99828817 -28.38683713 -30.03129832 -26.06504482]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [-26.52909933 -26.50962555 -26.00646373 -25.74092779]\n",
      " [-24.22991433 -25.50401748 -26.91235621 -24.02953407]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [  9.37131601   6.4824196   -1.14023037  -1.19815375]\n",
      " [  9.99997955   6.73056574   7.02359528  -0.50003813]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [-28.04100336 -20.01099163 -28.00803767 -27.52861208]\n",
      " [-27.24268874 -27.85496351 -27.73694774 -27.29772112]\n",
      " [-26.70488943 -27.88780762 -27.29050784 -28.35974003]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [-27.79828969 -29.41393067 -27.43543385 -27.27075827]\n",
      " [-26.47208645 -26.60050892 -28.67370679 -26.22673966]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [  2.81239587   3.20540633  -5.85853735   8.79691895]\n",
      " [  7.48940682 -13.5437284    8.02407507   8.89274048]\n",
      " [ -2.4003661   -2.92173444   7.82508123   9.9997436 ]\n",
      " [-26.1799587  -24.27160277 -29.16903839 -28.47393605]\n",
      " [-28.12853254 -16.77508085 -26.31630152 -29.11076561]\n",
      " [-26.50929423 -27.87400816 -28.4062906  -27.94210887]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [-27.08882585 -29.8528972  -28.66089775 -29.77761128]\n",
      " [-27.81223045 -15.58231813 -28.05304334 -27.29678092]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [  4.00993313 -15.75434377  -6.13151272   8.20864986]\n",
      " [  4.22144464 -12.41868327   7.67313284 -13.66488172]\n",
      " [ -6.43028238 -11.05876884  -1.23114492   9.24128161]\n",
      " [-25.59540015 -18.56122171 -27.23678946 -26.329822  ]\n",
      " [-14.62605448 -27.80509161 -24.83568014 -23.47897554]\n",
      " [-27.23893666 -16.25825554 -27.32000448 -30.00075846]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [-26.13602031  -3.70534836 -27.98997687 -26.11770447]\n",
      " [-28.4317736  -27.53920622 -22.62473976 -25.18761457]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [  6.74445067 -16.27901834  -5.95574251 -15.22086518]\n",
      " [-17.52587263 -17.6517387  -10.98014734   6.39511245]\n",
      " [-15.10707653 -14.71763974 -15.26017106   5.33181998]\n",
      " [-15.35783399 -24.22694173 -25.5645934  -24.09763338]\n",
      " [-24.99452947 -11.4720759  -26.65010169 -25.67150568]\n",
      " [-25.93260266 -27.75875683 -14.48831922 -27.39159101]\n",
      " [-23.19712403   1.68341743 -30.00404187 -23.53844087]\n",
      " [-22.45401949 -25.5674889   -1.92691459 -24.64057539]\n",
      " [-24.62533725   1.50873414 -25.76653097 -30.03983375]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [-16.87338314 -21.84029878 -17.7729012    5.17392388]\n",
      " [-20.09628587 -22.65624406 -22.5450256   -1.9838984 ]\n",
      " [-19.29322277  -0.16106696 -14.88870098 -10.36873451]\n",
      " [ -9.84837855 -29.42598255 -25.99191692 -25.92663934]\n",
      " [ -6.16666486 -27.41973222 -28.10825352 -24.75714142]\n",
      " [ -4.01040529 -25.03991668 -26.93741228 -29.81966525]\n",
      " [ -3.56300029 -25.97158364 -22.61903003 -26.13376199]\n",
      " [ -1.86087791 -24.86314097 -26.3453893  -24.12338531]\n",
      " [  1.67722462 -24.29599874 -25.67417407 -18.81273193]\n",
      " [-23.4690198    2.45012726 -21.34731921 -26.46837914]\n",
      " [-15.48332413 -22.7741225  -25.68050966   4.42578946]\n",
      " [-17.47544819 -15.0307638    4.36410734 -14.59775835]\n",
      " [-21.29403869 -24.56423406  -0.13344865 -15.68551552]\n",
      " [-27.51935942 -27.45604097 -27.83163199 -11.99097359]\n",
      " [-23.92066171 -26.16964668 -26.3030671  -26.14351302]\n",
      " [-28.24780371 -24.56996999 -27.86473062 -28.92087782]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [-14.08165198 -23.0646426  -26.31299844 -24.30520248]\n",
      " [-25.37517846 -21.71862237 -21.92331062  -7.80510795]\n",
      " [  3.18070982 -21.07253499 -26.78886676 -23.30114476]\n",
      " [-25.01757938 -21.95449117 -20.11464261   1.92375013]\n",
      " [-24.94317631 -25.42304776 -23.82217439   3.11717264]\n",
      " [-28.23513033 -28.31963937 -12.45908297 -26.00792409]\n",
      " [-24.55737685 -27.23907675 -27.04239379 -12.56059408]\n",
      " [-26.54259555 -30.71613689 -15.38965703 -26.91764763]\n",
      " [-27.60466065 -27.19873566 -14.47966493 -25.40312617]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [ -9.27961564 -23.89076274 -25.59724391 -26.21474839]\n",
      " [-24.33445307 -18.68689558 -22.09836852  -7.45429999]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [-15.15204436 -21.53280438 -26.38332279 -24.61784742]\n",
      " [-27.86836215 -23.61359976 -17.46842837  -4.92695137]\n",
      " [-28.87476051 -28.13974195 -26.86410062 -21.31679007]\n",
      " [-29.54753555 -30.61580694 -26.858352   -26.98214392]\n",
      " [-26.59205167 -27.8920543  -29.10898203 -26.95135291]\n",
      " [-27.71132442 -27.09033068 -28.01083602 -26.51542081]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [-25.90824355 -23.02249703 -24.5796893  -14.43957384]\n",
      " [-24.65584249 -21.9504493  -26.27889218 -10.20827998]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [-20.97074639 -28.4151602  -28.96905833 -26.64247276]\n",
      " [-28.42009866 -26.72318937 -29.8757355   -8.53883192]\n",
      " [-29.15023041 -27.69937355 -25.03684798 -28.54775517]\n",
      " [-28.99850846 -28.47497226 -28.94388227 -29.5404344 ]\n",
      " [-28.12114718 -29.0744331  -28.59935242 -26.4871259 ]\n",
      " [-28.2925088  -28.86404582 -27.59364236 -29.72431165]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [-24.14375308 -23.23306186 -23.97567375 -19.14334007]\n",
      " [-25.60237829 -24.22139834 -25.35972228 -17.91502734]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [-27.34426391 -28.39821476 -28.97883512 -28.38590971]\n",
      " [-27.50949713 -29.75365978 -30.71900746 -24.52253051]\n",
      " [-27.44424563 -27.4786451  -28.2784932  -25.18808881]]\n",
      "0->s\t -28.519742224955973 -27.194215495999337 -27.476616235168535 -28.25840333104418\n",
      "10->s\t -28.04100335535805 -20.010991631968167 -28.008037667122693 -27.528612078379375\n",
      "20->s\t -26.179958697300723 -24.271602771351848 -29.169038389561813 -28.473936048413965\n",
      "30->s\t -25.595400151834987 -18.561221714810756 -27.236789460799447 -26.32982200180553\n",
      "40->e\t -15.357833988457665 -24.22694173109494 -25.564593400537138 -24.097633381400847\n",
      "41->s\t -24.994529467845208 -11.472075900284393 -26.650101691165165 -25.67150568415\n",
      "51->e\t -6.166664864491739 -27.419732221129873 -28.108253524493325 -24.757141418458435\n",
      "52->e\t -4.010405292072886 -25.03991667879513 -26.937412278031047 -29.819665251049944\n",
      "53->e\t -3.563000292510833 -25.971583636345542 -22.619030028047437 -26.13376199147112\n",
      "54->e\t -1.8608779082770628 -24.863140973489273 -26.3453892987243 -24.12338531167704\n",
      "55->e\t 1.677224620951778 -24.29599874311069 -25.674174073088327 -18.812731928772223\n",
      "56->s\t -23.469019795401568 2.450127258886558 -21.34731921438796 -26.468379140800984\n",
      "66->e\t 3.1807098169352392 -21.072534989066487 -26.788866757458003 -23.301144760404053\n",
      "67->n\t -25.017579379720985 -21.954491165308973 -20.114642609173217 1.923750131123883\n",
      "57->n\t -15.483324126661016 -22.774122497044747 -25.68050965989479 4.425789464743186\n",
      "47->n\t -16.87338313620985 -21.840298779778923 -17.772901197832148 5.173923877656146\n",
      "37->e\t 6.744450672649847 -16.279018343809152 -5.9557425067459056 -15.220865175525283\n",
      "38->n\t -17.525872630610227 -17.65173870305401 -10.980147343101097 6.395112446757177\n",
      "28->w\t 4.221444638840291 -12.41868327013632 7.673132841202971 -13.664881722733416\n",
      "27->n\t 4.009933131686831 -15.754343767661604 -6.131512723447762 8.208649863287109\n",
      "17->n\t 2.812395868043986 3.2054063266975734 -5.858537353812542 8.79691894951768\n",
      "7->e\t 9.371316010094855 6.482419596733877 -1.140230368020294 -1.1981537519356753\n",
      "8->e\t 9.999979550144078 6.7305657390434925 7.023595280943812 -0.500038131900636\n"
     ]
    }
   ],
   "source": [
    "yuanyang = YuanYangEnv()\n",
    "brain = MC_RL(yuanyang)\n",
    "\n",
    "# 探索初始化方法\n",
    "qvalue1 = brain.mc_learning_ei(num_iter=1000000)\n",
    "\n",
    "# 将行为值函数渲染出来\n",
    "yuanyang.action_value = qvalue1\n",
    "print(qvalue1)\n",
    "\n",
    "# 测试学到的策略\n",
    "flag = 1\n",
    "s = 0\n",
    "# print(policy_value.pi)\n",
    "step_num = 0\n",
    "path = []\n",
    "\n",
    "# 将最优路径打印出来\n",
    "while flag:\n",
    "    # 渲染路径点\n",
    "    path.append(s)\n",
    "    yuanyang.path = path\n",
    "    a = brain.greedy_policy(qvalue1, s)\n",
    "    # a = agent.bolzman_policy(qvalue,s,0.1)\n",
    "    print('%d->%s\\t' % (s, a), qvalue1[s, 0], qvalue1[s, 1], qvalue1[s, 2], qvalue1[s, 3])\n",
    "    yuanyang.bird_male_position = yuanyang.state_to_position(s)\n",
    "    yuanyang.render()\n",
    "    time.sleep(0.25)\n",
    "    step_num += 1\n",
    "    s_, r, t = yuanyang.transform(s, a)\n",
    "    if t == True or step_num > 30:\n",
    "        flag = 0\n",
    "    s = s_\n",
    "    \n",
    "# 渲染最后的路径点\n",
    "yuanyang.bird_male_position = yuanyang.state_to_position(s)\n",
    "path.append(s)\n",
    "yuanyang.render()\n",
    "while True:\n",
    "    yuanyang.render()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
