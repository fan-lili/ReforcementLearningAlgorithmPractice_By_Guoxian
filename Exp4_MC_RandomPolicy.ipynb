{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8aa8829a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.2 (SDL 2.0.18, Python 3.9.9)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "# 同策略蒙特卡洛算法\n",
    "import random\n",
    "import time\n",
    "from yuanyangEnv import YuanYangEnv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d554cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MC_RL:\n",
    "    def __init__(self,yuanyang):\n",
    "        # 行为值函数的初始化\n",
    "        self.qvalue = np.ones((len(yuanyang.states),len(yuanyang.actions)))*0.1\n",
    "        \n",
    "        # 次数初始化，求经验平均时，q(s,a)=G(s,a)/n(s,a)\n",
    "        self.n = 0.001*np.ones((len(yuanyang.states),len(yuanyang.actions)))\n",
    "        self.actions = yuanyang.actions\n",
    "        self.gamma = yuanyang.gamma\n",
    "        self.yuanyang = yuanyang\n",
    "\n",
    "    def greedy_policy(self,qfun,state):\n",
    "        # 贪婪策略\n",
    "        amax = qfun[state,:].argmax()\n",
    "        return self.actions[amax]\n",
    "\n",
    "    def epsilon_greedy_policy(self,qfun,state,epsilon):\n",
    "        # ε-greedy策略\n",
    "        amax = qfun[state,:].argmax()\n",
    "        if np.random.uniform() < 1 - epsilon:\n",
    "            return self.greedy_policy(qfun,state)\n",
    "        else:\n",
    "            return self.actions[int(np.random.rand()*len(self.actions))]\n",
    "\n",
    "    def find_anum(self,a):\n",
    "        # 找到动作对应的序号\n",
    "        for i in range(len(self.actions)):\n",
    "            if a == self.actions[i]:\n",
    "                return i\n",
    "\n",
    "    def mc_learning_on_policy(self, num_iter, epsilon):\n",
    "        self.qvalue = np.zeros((len(yuanyang.states), len(yuanyang.actions)))\n",
    "        self.n = 0.001 * np.ones((len(yuanyang.states), len(yuanyang.actions)))\n",
    "        \n",
    "        # 学习num_iter次\n",
    "        for iter in range(num_iter):\n",
    "            # 采集状态样本\n",
    "            s_sample = []\n",
    "            \n",
    "            # 采集动作样本\n",
    "            a_sample = []\n",
    "            \n",
    "            # 采集回报样本\n",
    "            r_sample = []\n",
    "            \n",
    "            # #随机初始化状态\n",
    "            # s = self.yuanyang.reset()\n",
    "            # 固定初始状态\n",
    "            s = 0\n",
    "            done = False\n",
    "            step_num = 0\n",
    "            epsilon = epsilon * np.exp(-iter / 10)   # 探索率随迭代次数指数衰减\n",
    "            \n",
    "            # 采集数据s0-a1-s1-a2-s2...terminate state\n",
    "            # for i in range(5):\n",
    "            while False == done and step_num < 30:\n",
    "                a = self.epsilon_greedy_policy(self.qvalue, s, epsilon)\n",
    "                \n",
    "                # 与环境交互\n",
    "                s_next, r, done = self.yuanyang.transform(s, a)\n",
    "                a_num = self.find_anum(a)\n",
    "                \n",
    "                # 往回走给予惩罚\n",
    "                if s_next in s_sample:\n",
    "                    r = -2\n",
    "                    \n",
    "                # 存储数据，采样数据\n",
    "                s_sample.append(s)\n",
    "                r_sample.append(r)\n",
    "                a_sample.append(a_num)\n",
    "                step_num += 1\n",
    "                \n",
    "                # 转移到下一个状态，继续试验，s0-s1-s2\n",
    "                s = s_next\n",
    "                \n",
    "            # 任务完成结束条件\n",
    "            if s == 9:\n",
    "                print(\"同策略第一次完成任务需要的次数：\", iter)\n",
    "                break\n",
    "                \n",
    "            # 从样本中计算累计回报,g(s_0) = r_0+gamma*r_1+gamma^2*r_2+gamma^3*r3+v(sT)\n",
    "            a = self.epsilon_greedy_policy(self.qvalue, s, epsilon)\n",
    "            g = self.qvalue[s, self.find_anum(a)]\n",
    "            \n",
    "            # 计算该序列的第一状态的累计回报\n",
    "            for i in range(len(s_sample) - 1, -1, -1):\n",
    "                g *= self.gamma\n",
    "                g += r_sample[i]\n",
    "                \n",
    "            # print(\"episode number, trajectory\", iter1, s_sample)\n",
    "            # print(\"first state\", s_sample[0], g)\n",
    "            # g=G(s1,a),开始算其他状态处的累计回报\n",
    "            for i in range(len(s_sample)):\n",
    "                # 计算状态-行为对（s,a)的次数，s,a1...s,a2\n",
    "                self.n[s_sample[i], a_sample[i]] += 1.0\n",
    "                \n",
    "                # 利用增量式方法更新值函数\n",
    "                self.qvalue[s_sample[i], a_sample[i]] = (self.qvalue[s_sample[i], a_sample[i]] * (\n",
    "                            self.n[s_sample[i], a_sample[i]] - 1) + g) / self.n[s_sample[i], a_sample[i]]\n",
    "                g -= r_sample[i]\n",
    "                g /= self.gamma\n",
    "                # print(\"s_sample,a\",g)\n",
    "            # print(\"number\",self.n)\n",
    "        return self.qvalue\n",
    "\n",
    "    def mc_test(self):\n",
    "        s = 0\n",
    "        s_sample = []\n",
    "        done = False\n",
    "        flag = 0\n",
    "        step_num = 0\n",
    "        while False == done and step_num < 30:\n",
    "            a = self.greedy_policy(self.qvalue, s)\n",
    "            # 与环境交互\n",
    "            s_next, r, done = self.yuanyang.transform(s, a)\n",
    "            s_sample.append(s)\n",
    "            s = s_next\n",
    "            step_num += 1\n",
    "        if s == 9:\n",
    "            flag = 1\n",
    "        return flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcca5173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "同策略第一次完成任务需要的次数： 321\n",
      "[[-23.6780304  -16.52032294 -22.92689534 -23.2813468 ]\n",
      " [-23.20202635 -22.76585105 -26.43747283 -23.60202   ]\n",
      " [-20.10118267 -23.54247943 -24.84134137 -20.10118267]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [-25.24396996 -17.29970525 -23.38373204 -24.12261185]\n",
      " [-22.1869644  -21.51964885 -24.71239772 -24.84134137]\n",
      " [ -9.9950025  -15.37720048 -23.41433373 -23.86752329]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [-10.1413582    0.           0.           0.        ]\n",
      " [ -9.99000999 -11.63395702 -11.31734312   0.        ]\n",
      " [-23.14493774 -17.46239147 -21.89981112 -24.41830775]\n",
      " [-17.18889207 -17.29745574 -22.31708138 -22.78893624]\n",
      " [ -9.99000999  -8.50754246 -18.15302039 -17.05608206]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [-20.63288046 -20.63288046   0.           0.        ]\n",
      " [ -9.99000999 -20.01389936 -21.2209125    0.        ]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [-10.1413582    0.           0.           0.        ]\n",
      " [ -9.59040959 -11.63395702 -11.31734312   0.        ]\n",
      " [-14.7378858  -16.32200551 -15.7455711  -11.53048816]\n",
      " [-23.60849699 -18.11715418 -23.38373204 -22.93585704]\n",
      " [-22.88357632 -22.09715677 -24.01091124 -19.36774446]\n",
      " [ -9.99000999  -8.85014985 -23.21221866 -23.97674854]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [-18.27853869 -21.54911338  -9.99000999 -21.2209125 ]\n",
      " [-17.82455881 -21.06998132 -19.74995738 -19.74995738]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [-12.62796413   0.           0.           0.        ]\n",
      " [-19.23585222 -19.00255402 -11.47330485 -10.95420663]\n",
      " [-17.82455881 -17.26942635 -19.42425861 -16.63431744]\n",
      " [-15.83063709 -22.15068827 -21.89981112 -24.01091124]\n",
      " [-16.719556   -22.11239927 -21.66271886 -22.43487702]\n",
      " [-16.37390612 -23.2378283  -22.62074121 -22.87781122]\n",
      " [-17.12321886 -24.45226965 -20.6427871  -21.89981112]\n",
      " [-16.96821973 -20.78922324 -22.13302172 -21.20895813]\n",
      " [-20.10118267 -17.35091909 -21.45487304 -20.03964121]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [-20.38250024 -19.66978088  -9.99000999 -17.74560519]\n",
      " [-18.89843508 -20.68580857 -20.61853052 -18.63764726]\n",
      " [-17.82455881 -17.66955716 -19.26138367 -16.90279704]\n",
      " [-22.72402625 -22.68238436 -23.38373204 -26.17320399]\n",
      " [-20.8525305  -21.753257   -23.11058527 -23.18677068]\n",
      " [-18.97201042 -25.28302723 -21.19967069 -25.24658741]\n",
      " [-19.34489861 -17.82455881 -20.54838394 -24.5002354 ]\n",
      " [-19.70755443 -15.47059126 -20.6618148  -20.74773124]\n",
      " [-17.36154622 -20.40633115 -19.95184841 -21.06998132]\n",
      " [-17.75795765 -19.07699038 -20.53566836 -20.10118267]\n",
      " [-18.34641881 -19.28036284 -19.9122042  -19.31349994]\n",
      " [-17.99240645 -18.59892858 -19.57231034 -19.81435669]\n",
      " [-20.10118267 -22.36010888 -18.64926833 -17.98544425]\n",
      " [-23.12494604 -23.1328899  -23.38373204 -25.66567629]\n",
      " [-21.62056069 -23.76608901 -21.89376927 -22.09071124]\n",
      " [-21.89981112 -23.39397876 -21.60542117 -25.19213161]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [-19.75744149 -17.66880674 -17.82455881 -16.24590546]\n",
      " [-18.2378064  -13.52613982 -20.1097118  -20.75266673]\n",
      " [-19.02981453 -17.82455881 -18.60382954 -21.88074394]\n",
      " [-17.42649499 -13.70033354 -19.79819726 -22.58958339]\n",
      " [-18.17133932 -18.38989883 -18.61195709 -18.67126199]\n",
      " [ -9.99000999  -8.85014985 -19.02391335 -23.43081729]\n",
      " [-24.29359337 -23.18092279 -23.38373204 -25.69109291]\n",
      " [-25.25274861 -24.82413666 -24.69839786 -24.34309073]\n",
      " [-21.89981112 -22.02803439 -25.39826146 -22.8421028 ]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [ -9.21078921 -11.56358131  -9.99000999 -19.76017866]\n",
      " [ -9.99000999 -16.16384169 -12.13794844 -14.086153  ]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [-18.63764726 -16.75861169 -14.7378858  -14.086153  ]\n",
      " [ -9.59040959  -9.21078921 -19.00255402 -18.75666586]\n",
      " [ -9.99000999  -9.21078921   0.           0.        ]\n",
      " [-24.38214782 -24.19416446 -24.64509662 -24.69839786]\n",
      " [-22.31520799 -26.86777191 -25.29623609 -24.97116003]\n",
      " [-21.89981112 -20.64578487 -23.23246686 -22.13869026]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [-17.89883962  -9.21078921  -9.99000999 -12.89935393]\n",
      " [ -9.99000999  -9.59040959 -18.76508371   0.        ]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [ -8.85014985  -9.21078921  -9.99000999 -18.85112219]\n",
      " [ -9.59040959  -9.21078921   0.           0.        ]\n",
      " [ -9.99000999  -9.59040959   0.           0.        ]\n",
      " [-24.54367016 -23.38373204 -23.38373204 -25.32421342]\n",
      " [-24.40995115 -24.64509662 -25.29493096 -28.43153644]\n",
      " [-21.89981112 -21.89981112 -24.85260885 -20.98993042]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [ -9.59040959   0.           0.           0.        ]\n",
      " [ -9.99000999  -9.99000999   0.           0.        ]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [ -9.59040959   0.           0.           0.        ]\n",
      " [ -9.59040959  -9.99000999   0.           0.        ]\n",
      " [ -9.99000999  -9.99000999   0.           0.        ]]\n",
      "0->s\t -23.67803040146775 -16.52032294166406 -22.926895339784576 -23.28134679686977\n",
      "10->s\t -25.24396996345434 -17.299705254726963 -23.383732041575673 -24.122611848555426\n",
      "20->s\t -23.144937737779387 -17.462391474217966 -21.89981112107135 -24.41830774656601\n",
      "30->s\t -23.608496988486166 -18.117154184025402 -23.383732041575673 -22.935857038414984\n",
      "40->e\t -15.83063709073634 -22.150688269156422 -21.89981112107135 -24.010911236394225\n",
      "41->e\t -16.719555999072654 -22.112399270427066 -21.662718860957668 -22.43487702453331\n",
      "42->e\t -16.373906123365767 -23.237828299198966 -22.62074120619235 -22.87781121523385\n",
      "43->e\t -17.123218859527952 -24.452269654839345 -20.642787104027914 -21.89981112107135\n",
      "44->e\t -16.968219732449366 -20.789223237516037 -22.133021717884123 -21.20895812634393\n",
      "45->s\t -20.101182673495824 -17.350919090741215 -21.454873037639004 -20.039641209521903\n",
      "55->e\t -17.361546224940813 -20.406331150607173 -19.951848410772534 -21.069981316609486\n",
      "56->e\t -17.757957654303592 -19.076990378274544 -20.535668359981823 -20.101182673495824\n",
      "57->e\t -18.346418811837978 -19.280362844529478 -19.912204201053452 -19.313499935767585\n",
      "58->e\t -17.992406451176315 -18.598928576896242 -19.57231033939055 -19.814356693669566\n",
      "59->n\t -20.101182673495824 -22.36010888433549 -18.649268327482883 -17.98544424984287\n",
      "49->n\t -17.82455881464358 -17.669557155465284 -19.261383669124605 -16.902797043947373\n",
      "39->n\t -17.82455881464358 -17.269426353971006 -19.424258608825934 -16.6343174414904\n",
      "29->n\t -14.73788580235357 -16.322005509654467 -15.745571097722069 -11.530488158409819\n",
      "19->n\t -9.990009990009991 -11.633957015063627 -11.317343117379908 0.0\n"
     ]
    }
   ],
   "source": [
    "yuanyang = YuanYangEnv()\n",
    "brain = MC_RL(yuanyang)\n",
    "\n",
    "# on-policy方法\n",
    "qvalue = brain.mc_learning_on_policy(num_iter=1000000, epsilon=0.9)\n",
    "print(qvalue)\n",
    "\n",
    "# 将行为值函数渲染出来\n",
    "yuanyang.action_value = qvalue\n",
    "\n",
    "# 测试学到的策略\n",
    "flag = 1\n",
    "s = 0\n",
    "# print(policy_value.pi)\n",
    "step_num = 0\n",
    "path = []\n",
    "\n",
    "# 将最优路径打印出来\n",
    "while flag:\n",
    "    # 渲染路径点\n",
    "    path.append(s)\n",
    "    yuanyang.path = path\n",
    "    a = brain.greedy_policy(qvalue, s)\n",
    "    print('%d->%s\\t' % (s, a), qvalue[s, 0], qvalue[s, 1], qvalue[s, 2], qvalue[s, 3])\n",
    "    yuanyang.bird_male_position = yuanyang.state_to_position(s)\n",
    "    yuanyang.render()\n",
    "    time.sleep(0.25)\n",
    "    step_num += 1\n",
    "    s_, r, t = yuanyang.transform(s, a)\n",
    "    if t == True or step_num > 30:\n",
    "        flag = 0\n",
    "    s = s_\n",
    "    \n",
    "# 渲染最后的路径点\n",
    "yuanyang.bird_male_position = yuanyang.state_to_position(s)\n",
    "path.append(s)\n",
    "yuanyang.render()\n",
    "while True:\n",
    "    yuanyang.render()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
