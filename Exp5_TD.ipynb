{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f18fc0bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.2 (SDL 2.0.18, Python 3.9.9)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "# SARSA和Q-learning算法\n",
    "import time\n",
    "from yuanyangEnv import YuanYangEnv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bfafc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TD_RL:\n",
    "    def __init__(self,yuanyang):\n",
    "        self.yuanyang = yuanyang\n",
    "        self.qvalue = np.zeros((len(self.yuanyang.states), len(self.yuanyang.actions)))\n",
    "        self.gamma = yuanyang.gamma\n",
    "        self.actions = yuanyang.actions\n",
    "\n",
    "    def greedy_policy(self,qfun,state):\n",
    "        # 贪婪策略\n",
    "        amax = qfun[state,:].argmax()\n",
    "        return self.actions[amax]\n",
    "\n",
    "    def epsilon_greedy_policy(self,qfun,state,epsilon):\n",
    "        # ε-greedy策略\n",
    "        amax = qfun[state,:].argmax()\n",
    "        if np.random.uniform() < 1 - epsilon:\n",
    "            return self.yuanyang.actions[amax]\n",
    "        else:\n",
    "            return self.actions[int(np.random.rand()*len(self.actions))]\n",
    "\n",
    "    def find_anum(self,a):\n",
    "        # 找到动作对应的序号\n",
    "        for i in range(len(self.actions)):\n",
    "            if a == self.actions[i]:\n",
    "                return i\n",
    "\n",
    "    def greedy_test(self):\n",
    "        # 当测试初始状态为0时，采用当前的贪婪策略能否找到目标点\n",
    "        s = 0\n",
    "        s_sample = []\n",
    "        done = False\n",
    "        flag = 0\n",
    "        step_num = 0\n",
    "        while False == done and step_num < 30:\n",
    "            a = self.greedy_policy(self.qvalue,s)\n",
    "            # 与环境交互\n",
    "            s_next, r, done = self.yuanyang.transform(s, a)\n",
    "            s_sample.append(s)\n",
    "            s = s_next\n",
    "            step_num += 1\n",
    "        if s == 9:\n",
    "            flag = 1\n",
    "        if s == 9 and step_num < 21:\n",
    "            # 找到最短路径\n",
    "            flag = 2\n",
    "        return flag\n",
    "\n",
    "    def sarsa(self,num_iter,alpha,epsilon):\n",
    "        # 同策略时间差分强化学习算法SARSA\n",
    "        iter_num = []\n",
    "        self.qvalue = np.zeros((len(yuanyang.states),len(yuanyang.actions)))\n",
    "        \n",
    "        # 外循环\n",
    "        for iter in range(num_iter):\n",
    "            epsilon = epsilon * 0.99\n",
    "            s_sample = []\n",
    "            s = 0\n",
    "            flag = self.greedy_test()\n",
    "            if flag == 1:\n",
    "                iter_num.append(iter)\n",
    "                if len(iter_num) < 2:   # 第一次找到终点\n",
    "                    print('sarsa第一次完成任务需要的迭代次数为:',iter_num[0])\n",
    "            if flag == 2:\n",
    "                print('sarsa第一次实现最短路径需要的迭代次数为:', iter)\n",
    "                break\n",
    "                \n",
    "            # epsilon_greedy策略选择动作\n",
    "            a = self.epsilon_greedy_policy(self.qvalue, s, epsilon)\n",
    "            done = False\n",
    "            count = 0\n",
    "            # 第二个循环，一个实验，s0-s1-s2-s1-s2-s_terminate\n",
    "            while False == done and count < 30:\n",
    "                # 与环境交互\n",
    "                s_next, r, done = self.yuanyang.transform(s, a)\n",
    "                a_num = self.find_anum(a)\n",
    "                if s_next in s_sample:\n",
    "                    r = -2\n",
    "                s_sample.append(s)\n",
    "                \n",
    "                # 判断一下,是否是终止状态\n",
    "                if done == True:\n",
    "                    q_target = r\n",
    "                else:\n",
    "                    # 下一个状态处的最大动作,这个地方体现on-policy\n",
    "                    a1 = self.epsilon_greedy_policy(self.qvalue, s_next, epsilon)\n",
    "                    a1_num = self.find_anum(a1)\n",
    "                    # 更新公式\n",
    "                    q_target = r + self.gamma * self.qvalue[s_next, a1_num]\n",
    "                    \n",
    "                # 利用td方法更新动作值函数，alpha\n",
    "                self.qvalue[s, a_num] = self.qvalue[s, a_num] + alpha * (q_target - self.qvalue[s, a_num])\n",
    "                \n",
    "                s = s_next\n",
    "                a = self.epsilon_greedy_policy(self.qvalue, s, epsilon)\n",
    "                count += 1\n",
    "        return self.qvalue\n",
    "\n",
    "    def qlearning(self, num_iter, alpha, epsilon):\n",
    "        iter_num = []\n",
    "        self.qvalue = np.zeros((len(self.yuanyang.states), len(self.yuanyang.actions)))\n",
    "        \n",
    "        # 外循环\n",
    "        for iter in range(num_iter):\n",
    "            s = 0\n",
    "            flag = self.greedy_test()\n",
    "            if flag == 1:\n",
    "                iter_num.append(iter)\n",
    "                if len(iter_num) < 2:\n",
    "                    print(\"qlearning 第一次完成任务需要的迭代次数为：\", iter_num[0])\n",
    "            if flag == 2:\n",
    "                print(\"qlearning 第一次实现最短路径需要的迭代次数为：\", iter)\n",
    "                break\n",
    "                \n",
    "            s_sample = []\n",
    "            a = self.epsilon_greedy_policy(self.qvalue, s, epsilon)\n",
    "            done = False\n",
    "            count = 0\n",
    "            while False == done and count < 30:\n",
    "                # 与环境交互得到下一个状态\n",
    "                s_next, r, done = yuanyang.transform(s, a)\n",
    "                a_num = self.find_anum(a)\n",
    "                \n",
    "                if s_next in s_sample:\n",
    "                    r = -2\n",
    "                s_sample.append(s)\n",
    "                \n",
    "                if done == True:\n",
    "                    q_target = r\n",
    "                else:\n",
    "                    # 下一个状态处的最大动作，a1用greedy_policy，与SARSA不同\n",
    "                    a1 = self.greedy_policy(self.qvalue, s_next)\n",
    "                    a1_num = self.find_anum(a1)\n",
    "                    # qlearning的更新公式TD(0)\n",
    "                    q_target = r + self.gamma * self.qvalue[s_next, a1_num]\n",
    "                \n",
    "                # 利用td方法更新动作值函数\n",
    "                self.qvalue[s, a_num] = self.qvalue[s, a_num] + alpha * (q_target - self.qvalue[s, a_num])\n",
    "                \n",
    "                s = s_next\n",
    "                a = self.epsilon_greedy_policy(self.qvalue, s, epsilon)\n",
    "                count += 1\n",
    "        return self.qvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883d6692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sarsa第一次实现最短路径需要的迭代次数为: 232\n",
      "[[-1.83653111 -1.65476535 -8.58391693 -7.94964431]\n",
      " [-1.40380891 -1.38621989 -1.91890742 -5.09515759]\n",
      " [-4.10239    -1.54973145 -1.70573084 -3.636559  ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-0.21166475 -0.1936822  -0.38       -1.        ]\n",
      " [-1.         -0.475855   -0.24891075 -1.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-0.01        0.          0.          0.        ]\n",
      " [ 1.9         0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-1.68275503 -1.73883239 -4.5872479  -1.98980704]\n",
      " [-1.60819123 -1.39456528 -1.79032219 -1.5671727 ]\n",
      " [-3.2951     -1.4083076  -1.62413356 -1.52157753]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-0.246639   -0.2133021  -0.38       -0.209     ]\n",
      " [-1.9        -0.21917148 -0.4071     -0.28470941]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-0.01       -0.201805   -0.2        -0.01      ]\n",
      " [-0.01       -0.20095    -0.01       -0.01      ]\n",
      " [-1.         -0.20095    -0.01        0.        ]\n",
      " [-1.60050011 -1.63779018 -4.98064231 -1.76808773]\n",
      " [-1.73838861 -1.30036828 -1.86420864 -1.70210091]\n",
      " [-5.17576231 -1.14768659 -1.40140829 -1.38150726]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-0.42137658 -0.59480584 -0.38       -0.23455198]\n",
      " [-1.9        -0.4012268  -0.38725827 -0.20597556]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-0.06166575 -0.20095    -1.9        -0.2171    ]\n",
      " [-0.0271     -0.20095    -0.20995    -0.019     ]\n",
      " [-1.9        -0.20095    -0.01095    -0.209     ]\n",
      " [-1.54511655 -1.41116198 -4.0951     -1.48820497]\n",
      " [-1.07178605 -1.52102533 -1.58335102 -1.66135997]\n",
      " [-6.79306712 -0.89492556 -1.07046732 -1.26771371]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-0.47853134 -0.58495631 -1.1        -0.50704879]\n",
      " [-1.         -0.4108343  -0.73361008 -0.25568428]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-0.209      -0.201805   -0.2        -0.06696077]\n",
      " [-0.209      -0.38429353 -0.2111413  -0.01      ]\n",
      " [-1.         -0.295      -0.01095    -0.209     ]\n",
      " [-1.17296807 -1.28549552 -1.99       -1.2824267 ]\n",
      " [-1.38033916 -1.21624662 -1.2705009  -1.41416133]\n",
      " [-0.77713226 -0.99633578 -1.23709377 -0.99143008]\n",
      " [-0.70530011 -0.68474411 -1.15651945 -1.9       ]\n",
      " [-0.53413423 -0.50365085 -0.76654783 -0.63702501]\n",
      " [-1.         -0.76095302 -0.74791695 -0.44277141]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-0.389905   -0.38632851 -1.9        -0.19761993]\n",
      " [-0.077051   -0.20095    -0.06464875 -0.2335255 ]\n",
      " [-1.         -0.39009025 -0.201805   -0.304     ]\n",
      " [-0.93379338 -1.14782719 -2.791      -1.544633  ]\n",
      " [-1.05172092 -0.96584458 -1.09668463 -1.17967321]\n",
      " [-0.92913544 -1.08601855 -0.88415843 -0.99203429]\n",
      " [-0.53256875 -2.6471     -0.97316473 -0.74661133]\n",
      " [-0.40470306 -0.51535883 -0.87681072 -0.57210727]\n",
      " [-0.30844421 -0.33660312 -0.57086756 -0.36279941]\n",
      " [-0.19163404 -0.47977407 -0.37962947 -1.        ]\n",
      " [-0.29569403 -0.21976    -0.21171466 -0.16134654]\n",
      " [-0.20539    -0.08697958 -0.39109601 -0.07157257]\n",
      " [-1.1        -0.3916245  -0.56097383 -0.21805   ]\n",
      " [-1.18967495 -1.01330465 -1.1        -1.13289965]\n",
      " [-1.05784956 -0.81914563 -1.14754488 -1.17878691]\n",
      " [-1.9        -1.12364274 -1.2192285  -1.14915225]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-0.61897434 -0.50860361 -1.         -0.44349936]\n",
      " [-0.29645944 -0.43770384 -0.47956326 -0.4011404 ]\n",
      " [-0.18645834 -1.1        -0.399855   -0.21193132]\n",
      " [-0.22439    -0.05651248 -0.201805   -0.0592031 ]\n",
      " [-0.1981     -0.22534    -0.38266    -0.3850716 ]\n",
      " [-0.38       -0.20995    -0.2        -0.22722148]\n",
      " [-0.83124945 -0.81546506 -1.9        -0.79733409]\n",
      " [-0.72687523 -0.72054344 -0.71747655 -0.80761731]\n",
      " [-1.99       -0.89135825 -0.96006915 -1.06646796]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-0.23625003 -0.2212854  -1.         -0.44217782]\n",
      " [-1.1        -0.24073    -0.39900756 -0.21083   ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-0.028905   -0.22534    -1.         -0.21565084]\n",
      " [-0.2171     -0.21805    -0.21019458 -0.2025745 ]\n",
      " [-1.         -0.21805    -0.201805   -0.2       ]\n",
      " [-1.16201935 -1.04238546 -1.1        -1.14149239]\n",
      " [-0.99890244 -1.20045641 -1.10279759 -1.15451708]\n",
      " [-1.1        -1.14611221 -1.17662266 -1.02751819]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-0.02983    -0.22534    -1.         -0.2033573 ]\n",
      " [-1.         -0.30229    -0.475      -0.204199  ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-0.019      -0.01       -1.         -0.2025745 ]\n",
      " [-0.019      -0.01       -0.01       -0.011805  ]\n",
      " [-1.         -0.019      -0.01       -0.011805  ]\n",
      " [-1.10310378 -1.91       -1.9        -1.20082763]\n",
      " [-1.24112283 -2.062      -1.182453   -1.29056901]\n",
      " [-1.18       -1.19       -1.28849985 -1.13618778]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-0.3121     -1.         -1.         -0.2025745 ]\n",
      " [-1.         -1.         -0.210805   -0.2376273 ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.         -1.          0.        ]\n",
      " [-0.01       -1.          0.          0.        ]\n",
      " [-1.         -1.         -0.01        0.        ]]\n",
      "0->s\t -1.8365311111922327 -1.6547653523684982 -8.583916926087086 -7.949644310033343\n",
      "10->e\t -1.6827550283359312 -1.7388323904337915 -4.5872479 -1.9898070371394287\n",
      "11->s\t -1.608191234891366 -1.3945652762589553 -1.7903221945318346 -1.56717270201806\n",
      "21->s\t -1.7383886103145831 -1.3003682812934616 -1.8642086391257504 -1.7021009098930275\n",
      "31->e\t -1.0717860522756724 -1.521025334820751 -1.583351015717819 -1.6613599739901546\n",
      "32->s\t -6.7930671151899995 -0.8949255636364817 -1.0704673193003076 -1.2677137134809793\n",
      "42->e\t -0.7771322634190396 -0.9963357769560475 -1.2370937738603662 -0.9914300805515324\n",
      "43->s\t -0.7053001076774128 -0.6847441069511907 -1.1565194475451002 -1.9\n",
      "53->e\t -0.5325687493558663 -2.6471 -0.9731647278450948 -0.7466113253358121\n",
      "54->e\t -0.40470305957408753 -0.5153588274005807 -0.8768107179563714 -0.5721072672273062\n",
      "55->e\t -0.3084442082876539 -0.3366031224762084 -0.570867560978075 -0.3627994110194348\n",
      "56->e\t -0.19163404337067694 -0.479774065485 -0.37962946750000004 -1.0\n",
      "57->n\t -0.29569403263188904 -0.21976000000000004 -0.21171466300000003 -0.16134653551750003\n",
      "47->n\t -0.38990500000000006 -0.38632851050000006 -1.9 -0.197619930525\n",
      "37->n\t -0.20900000000000002 -0.201805 -0.2 -0.06696076900000002\n",
      "27->e\t -0.061665750000000005 -0.20095000000000002 -1.9 -0.21710000000000002\n",
      "28->n\t -0.027100000000000006 -0.20095000000000002 -0.20995000000000003 -0.019000000000000003\n",
      "18->e\t -0.010000000000000002 -0.20095000000000002 -0.010000000000000002 -0.010000000000000002\n",
      "19->n\t -1.0 -0.20095000000000002 -0.010000000000000002 0.0\n"
     ]
    }
   ],
   "source": [
    "yuanyang = YuanYangEnv()\n",
    "TD = TD_RL(yuanyang)\n",
    "\n",
    "# on-policy方法\n",
    "qvalue = TD.sarsa(num_iter=5000,alpha=0.1,epsilon=0.8)    # 注意yuanyang中的无效奖励改变为-0.1\n",
    "# qvalue = TD.qlearning(num_iter=5000,alpha=0.1,epsilon=0.1)\n",
    "print(qvalue)\n",
    "\n",
    "# 将行为值函数渲染出来\n",
    "yuanyang.action_value = qvalue\n",
    "\n",
    "# 测试学到的策略\n",
    "flag = 1\n",
    "s = 0\n",
    "step_num = 0\n",
    "path = []\n",
    "\n",
    "# 将最优路径打印出来\n",
    "while flag:\n",
    "    # 渲染路径点\n",
    "    path.append(s)\n",
    "    yuanyang.path = path\n",
    "    a = TD.greedy_policy(qvalue, s)\n",
    "    print('%d->%s\\t' % (s, a), qvalue[s, 0], qvalue[s, 1], qvalue[s, 2], qvalue[s, 3])\n",
    "    yuanyang.bird_male_position = yuanyang.state_to_position(s)\n",
    "    yuanyang.render()\n",
    "    time.sleep(0.25)\n",
    "    step_num += 1\n",
    "    s_, r, t = yuanyang.transform(s, a)\n",
    "    if t == True or step_num > 30:\n",
    "        flag = 0\n",
    "    s = s_\n",
    "    \n",
    "# 渲染最后的路径点\n",
    "yuanyang.bird_male_position = yuanyang.state_to_position(s)\n",
    "path.append(s)\n",
    "yuanyang.render()\n",
    "while True:\n",
    "    yuanyang.render()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
