{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a60d065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.2 (SDL 2.0.18, Python 3.9.9)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "# 基于表格特征和固定系数形式的Q-Learning\n",
    "from yuanyangEnv import YuanYangEnv\n",
    "import numpy as np\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e8e1555",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LFA_RL:\n",
    "    def __init__(self, yuanyang):\n",
    "        self.gamma = yuanyang.gamma\n",
    "        self.yuanyang = yuanyang\n",
    "        self.theta_tr =  np.zeros((400,1))*0.1     # 表格特征表示\n",
    "        self.theta_fsr = np.zeros((80,1))*0.1      # 固定稀疏表示\n",
    "\n",
    "    # 找到动作所对应的序号\n",
    "    def find_anum(self, a):\n",
    "        for i in range(len(self.yuanyang.actions)):\n",
    "            if a == self.yuanyang.actions[i]:\n",
    "                return i\n",
    "\n",
    "    # 实现表格特征表示\n",
    "    def feature_tr(self,s,a):\n",
    "        phi_s_a = np.zeros((1,400))\n",
    "        phi_s_a[0, 100*a+s] = 1\n",
    "        return phi_s_a\n",
    "\n",
    "    # 定义贪婪策略\n",
    "    def greedy_policy_tr(self,state):\n",
    "        qfun = np.array([0,0,0,0])*0.1\n",
    "        # 计算行为值函数Q(s,a)=phi(s,a)*theta\n",
    "        for i in range(4):\n",
    "            qfun[i] = np.dot(self.feature_tr(state,i),self.theta_tr)\n",
    "        amax=qfun.argmax()\n",
    "        return self.yuanyang.actions[amax]\n",
    "\n",
    "    # 定义epsilon贪婪策略\n",
    "    def epsilon_greedy_policy_tr(self, state, epsilon):\n",
    "        qfun = np.array([0, 0, 0, 0])*0.1\n",
    "        # 计算行为值函数Q(s,a)=phi(s,a)*theta\n",
    "        for i in range(4):\n",
    "            qfun[i] = np.dot(self.feature_tr(state, i), self.theta_tr)\n",
    "        amax = qfun.argmax()\n",
    "        # 概率部分\n",
    "        if np.random.uniform() < 1 - epsilon:\n",
    "            # 最优动作\n",
    "            return self.yuanyang.actions[amax]\n",
    "        else:\n",
    "            return self.yuanyang.actions[int(random.random() * len(self.yuanyang.actions))]\n",
    "\n",
    "    # 定义贪婪策略\n",
    "    def greedy_test_tr(self):\n",
    "        s = 0\n",
    "        s_sample = []\n",
    "        done = False\n",
    "        flag = 0\n",
    "        step_num = 0\n",
    "        while False == done and step_num < 30:\n",
    "            a = self.greedy_policy_tr(s)\n",
    "            # 与环境交互\n",
    "            s_next, r, done = self.yuanyang.transform(s, a)\n",
    "            s_sample.append(s)\n",
    "            s = s_next\n",
    "            step_num += 1\n",
    "        if s == 9:\n",
    "            flag = 1\n",
    "        if s == 9 and step_num < 21:\n",
    "            flag = 2\n",
    "        return flag\n",
    "\n",
    "    def qlearning_lfa_tr(self,num_iter, alpha, epsilon):\n",
    "        # 表格特征表示\n",
    "        iter_num = []\n",
    "        self.theta_tr = np.zeros((400, 1)) * 0.1\n",
    "        \n",
    "        #大循环\n",
    "        for iter in range(num_iter):\n",
    "            s = 0\n",
    "            flag = self.greedy_test_tr()\n",
    "            if flag == 1:\n",
    "                iter_num.append(iter)\n",
    "                if len(iter_num)<2:\n",
    "                    print(\"qlearning_tr 第一次完成任务需要的迭代次数为：\", iter_num[0])\n",
    "            if flag == 2:\n",
    "                print(\"qlearning_tr 第一次实现最短路径需要的迭代次数为：\", iter)\n",
    "                break\n",
    "                \n",
    "            s_sample = []\n",
    "            a = self.epsilon_greedy_policy_tr(s,epsilon)\n",
    "            done = False\n",
    "            count = 0\n",
    "            while False==done and count < 30:\n",
    "                # 与环境交互\n",
    "                s_next, r, done = yuanyang.transform(s, a)\n",
    "                a_num = self.find_anum(a)\n",
    "                if s_next in s_sample:\n",
    "                    r = -2\n",
    "                s_sample.append(s)\n",
    "                if done == True:\n",
    "                    q_target = r\n",
    "                else:\n",
    "                    # 下一个状态处的最大动作，a1用greedy_policy\n",
    "                    a1 = self.greedy_policy_tr(s_next)\n",
    "                    a1_num = self.find_anum(a1)\n",
    "                    \n",
    "                    # qlearning的更新公式TD(0)\n",
    "                    q_target = r + self.gamma * np.dot(self.feature_tr(s_next, a1_num),self.theta_tr)\n",
    "                    # print(\"q_target\", q_target[0,0],np.sum(self.feature(s,a_num)))\n",
    "                    \n",
    "                # 利用td方法更新动作值函数\n",
    "                self.theta_tr= self.theta_tr + alpha * (q_target - np.dot(self.feature_tr(s,a_num),self.theta_tr))[0,0]*np.transpose(self.feature_tr(s,a_num))\n",
    "                \n",
    "                s = s_next\n",
    "                a = self.epsilon_greedy_policy_tr(s, epsilon)\n",
    "                count += 1\n",
    "        return self.theta_tr\n",
    "\n",
    "    # 固定稀疏表示\n",
    "    def feature_fsr(self,s,a):\n",
    "        phi_s_a = np.zeros((1,80))\n",
    "        y = int(s/10)\n",
    "        x = s-10*y\n",
    "        phi_s_a[0,20*a+x] = 1\n",
    "        phi_s_a[0,20*a+10+y] = 1\n",
    "        return phi_s_a\n",
    "\n",
    "    def greedy_policy_fsr(self, state):\n",
    "        qfun = np.array([0, 0, 0, 0]) * 0.1\n",
    "        # 计算行为值函数Q(s,a)=phi(s,a)*theta\n",
    "        for i in range(4):\n",
    "            qfun[i] = np.dot(self.feature_fsr(state, i), self.theta_fsr)\n",
    "        amax = qfun.argmax()\n",
    "        return self.yuanyang.actions[amax]\n",
    "\n",
    "    # 定义epsilon贪婪策略\n",
    "    def epsilon_greedy_policy_fsr(self, state, epsilon):\n",
    "        qfun = np.array([0, 0, 0, 0]) * 0.1\n",
    "        # 计算行为值函数Q(s,a)=phi(s,a)*theta\n",
    "        for i in range(4):\n",
    "            qfun[i] = np.dot(self.feature_fsr(state, i), self.theta_fsr)\n",
    "        amax = qfun.argmax()\n",
    "        # 概率部分\n",
    "        if np.random.uniform() < 1 - epsilon:\n",
    "            # 最优动作\n",
    "            return self.yuanyang.actions[amax]\n",
    "        else:\n",
    "            return self.yuanyang.actions[int(random.random() * len(self.yuanyang.actions))]\n",
    "\n",
    "    def greedy_test_fsr(self):\n",
    "        s = 0\n",
    "        s_sample = []\n",
    "        done = False\n",
    "        flag = 0\n",
    "        step_num = 0\n",
    "        while False == done and step_num < 30:\n",
    "            a = self.greedy_policy_fsr(s)\n",
    "            # 与环境交互\n",
    "            s_next, r, done = self.yuanyang.transform(s, a)\n",
    "            s_sample.append(s)\n",
    "            s = s_next\n",
    "            step_num += 1\n",
    "        if s == 9:\n",
    "            flag = 1\n",
    "        if s == 9 and step_num < 21:\n",
    "            flag = 2\n",
    "        return flag\n",
    "\n",
    "    def qlearning_lfa_fsr(self, num_iter, alpha, epsilon):\n",
    "        iter_num = []\n",
    "        self.theta_fsr = np.zeros((80, 1)) * 0.1\n",
    "        \n",
    "        # 大循环\n",
    "        for iter in range(num_iter):\n",
    "            s = 0\n",
    "            flag = self.greedy_test_fsr()\n",
    "            if flag == 1:\n",
    "                iter_num.append(iter)\n",
    "                if len(iter_num) < 2:\n",
    "                    print(\"qlearning_fsr 第一次完成任务需要的迭代次数为：\", iter_num[0])\n",
    "            if flag == 2:\n",
    "                print(\"qlearning_fsr 第一次实现最短路径需要的迭代次数为：\", iter)\n",
    "                break\n",
    "                \n",
    "            s_sample = []\n",
    "            a = self.epsilon_greedy_policy_fsr(s, epsilon)\n",
    "            t = False\n",
    "            count = 0\n",
    "            while False == t and count < 30:\n",
    "                # 与环境交互得到下一个状态\n",
    "                s_next, r, t = yuanyang.transform(s, a)\n",
    "                # print(s)\n",
    "                # print(s_next)\n",
    "                a_num = self.find_anum(a)\n",
    "                if s_next in s_sample:\n",
    "                    r = -2\n",
    "                s_sample.append(s)\n",
    "                if t == True:\n",
    "                    q_target = r\n",
    "                else:\n",
    "                    # 下一个状态处的最大动作，a1用greedy_policy\n",
    "                    a1 = self.greedy_policy_fsr(s_next)\n",
    "                    a1_num = self.find_anum(a1)\n",
    "                    \n",
    "                    # 得到时间差分目标\n",
    "                    q_target = r + self.gamma * np.dot(self.feature_fsr(s_next, a1_num), self.theta_fsr)\n",
    "                    # print(\"q_target\", q_target[0,0],np.sum(self.feature(s,a_num)))\n",
    "                    \n",
    "                # 基于时间差分目标和梯度下降法更新行为值函数的参数\n",
    "                self.theta_fsr = self.theta_fsr + alpha * (q_target - np.dot(self.feature_fsr(s, a_num), self.theta_fsr))[\n",
    "                    0, 0] * np.transpose(self.feature_fsr(s, a_num))\n",
    "                \n",
    "                s = s_next\n",
    "                a = self.epsilon_greedy_policy_fsr(s, epsilon)\n",
    "                count += 1\n",
    "        return self.theta_fsr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f07ad00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qlearning_fsr 第一次实现最短路径需要的迭代次数为： 237\n",
      "qlearning_tr 第一次完成任务需要的迭代次数为： 217\n",
      "qlearning_tr 第一次实现最短路径需要的迭代次数为： 267\n",
      "0->s\t -7.001650686926691 -5.407264150900307 -9.858053387855843 -8.36570245663733\n",
      "10->s\t -7.1708925708436695 -5.495017015704324 -9.81719901057815 -7.7501295519126465\n",
      "20->s\t -7.487004997630587 -5.466242323454753 -10.016906323857292 -7.7263343587237925\n",
      "30->s\t -7.584531291136639 -5.726416798516965 -10.217971294449276 -7.858799525695746\n",
      "40->s\t -7.690195739689238 -5.7937917200242826 -11.298184621937661 -8.122144956353463\n",
      "50->e\t -5.395095098772372 -6.572299445251793 -11.525131516287631 -8.48555769427172\n",
      "51->e\t -5.3076251608476674 -8.229022590758204 -8.84299001475964 -7.948449424577646\n",
      "52->e\t -5.002866063836322 -8.899602533130313 -9.355531029117017 -8.619195446676354\n",
      "53->e\t -4.185875378148872 -8.321946737612542 -7.580073920851797 -7.514753046411187\n",
      "54->e\t -3.7997129986608793 -8.282438751440901 -8.54527710240969 -7.46785546012846\n",
      "55->e\t -3.8478029739553756 -7.47292007097703 -8.694321909167432 -7.495555834721004\n",
      "56->e\t -3.869220395589549 -4.126055159971455 -4.6838752660362974 -4.258887819493288\n",
      "57->e\t -4.6082179441883575 -6.277583542798821 -7.656966230085933 -5.693401846171364\n",
      "58->e\t -5.124631965314631 -6.13920407775378 -6.296386997241866 -5.507344055145731\n",
      "59->n\t -5.837109800460646 -6.0519295695351465 -5.958949128974033 -4.941108935775209\n",
      "49->n\t -8.132210441377513 -5.273421844307636 -5.732002234624062 -4.577696197856953\n",
      "39->n\t -8.026545992824914 -5.2060469228003186 -4.6517889071356775 -4.314350767199235\n",
      "29->n\t -7.929019699318861 -4.945872447738107 -4.450723936543692 -4.181885600227282\n",
      "19->n\t -7.6129072725319435 -4.9746471399876775 -4.251016623264551 -4.205680793416136\n"
     ]
    }
   ],
   "source": [
    "yuanyang = YuanYangEnv()\n",
    "brain = LFA_RL(yuanyang)\n",
    "brain.qlearning_lfa_fsr(num_iter=5000,alpha=0.1,epsilon=0.1)\n",
    "brain.qlearning_lfa_tr(num_iter=5000, alpha=0.1, epsilon=0.1)\n",
    "\n",
    "#打印学到的值函数\n",
    "qvalue2 =  np.zeros((100,4))\n",
    "qvalue1 = np.zeros((100,4))\n",
    "for i in range(400):\n",
    "    y = int(i/100)\n",
    "    x = i-100*y\n",
    "    qvalue2[x,y] = np.dot(brain.feature_tr(x,y),brain.theta_tr)\n",
    "    qvalue1[x,y] = np.dot(brain.feature_fsr(x,y),brain.theta_fsr)\n",
    "yuanyang.action_value = qvalue1\n",
    "\n",
    "# 测试学到的策略\n",
    "flag = 1\n",
    "s = 0\n",
    "# print(policy_value.pi)\n",
    "step_num = 0\n",
    "path = []\n",
    "\n",
    "# 将最优路径打印出来\n",
    "while flag:\n",
    "    # 渲染路径点\n",
    "    path.append(s)\n",
    "    yuanyang.path = path\n",
    "    # a = brain.greedy_policy_tr(s)\n",
    "    a = brain.greedy_policy_fsr(s)\n",
    "    print('%d->%s\\t' % (s, a), qvalue1[s, 0], qvalue1[s, 1], qvalue1[s, 2], qvalue1[s, 3])\n",
    "    yuanyang.bird_male_position = yuanyang.state_to_position(s)\n",
    "    yuanyang.render()\n",
    "    time.sleep(0.25)\n",
    "    step_num += 1\n",
    "    s_, r, t = yuanyang.transform(s, a)\n",
    "    if t == True or step_num > 30:\n",
    "        flag = 0\n",
    "    s = s_\n",
    "    \n",
    "# 渲染最后的路径点\n",
    "yuanyang.bird_male_position = yuanyang.state_to_position(s)\n",
    "path.append(s)\n",
    "yuanyang.render()\n",
    "while True:\n",
    "    yuanyang.render()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
