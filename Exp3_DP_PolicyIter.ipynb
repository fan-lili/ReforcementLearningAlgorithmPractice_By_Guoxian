{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc9acb1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.2 (SDL 2.0.18, Python 3.9.9)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "# 策略迭代\n",
    "import random\n",
    "import time\n",
    "from yuanyangEnv import YuanYangEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "747973c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DP_Policy_Iter:\n",
    "    def __init__(self,yuanyang):\n",
    "        self.states = yuanyang.states\n",
    "        self.actions = yuanyang.actions\n",
    "        self.v = [0.0 for i in range(len(self.states)+1)]\n",
    "        self.pi = dict()    # 策略\n",
    "        self.yuanyang = yuanyang\n",
    "        self.gamma = yuanyang.gamma\n",
    "\n",
    "        # 初始化策略\n",
    "        for state in self.states:\n",
    "            flag1 = 0\n",
    "            flag2 = 0\n",
    "            flag1 = yuanyang.collide(yuanyang.state_to_position(state))\n",
    "            flag2 = yuanyang.find(yuanyang.state_to_position(state))\n",
    "            if flag1 == 1 or flag2 == 1:\n",
    "                continue\n",
    "            self.pi[state] = self.actions[int(random.random()*len(self.actions))]   # 每个状态下随机选取一个动作\n",
    "\n",
    "    def policy_evaluate(self):\n",
    "        # 策略评估，再计算值函数\n",
    "            # 内层循环遍历状态空间中的每个状态，利用贝尔曼算子更新每个状态处的值函数；\n",
    "            # 外循环迭代计算整个值函数\n",
    "            # 当累计和小于1e-06时，说明值函数收敛，退出程序\n",
    "        for i in range(100):    # 迭代100次\n",
    "            delta = 0.0\n",
    "            for state in self.states:\n",
    "                flag1 = 0\n",
    "                flag2 = 0\n",
    "                flag1 = yuanyang.collide(yuanyang.state_to_position(state))\n",
    "                flag2 = yuanyang.find(yuanyang.state_to_position(state))\n",
    "                if flag1 == 1 or flag2 == 1:\n",
    "                    continue\n",
    "                    \n",
    "                action = self.pi[state]\n",
    "                s,r,t = yuanyang.transform(state,action)\n",
    "                \n",
    "                # 更新值\n",
    "                new_v = r + self.gamma * self.v[s]\n",
    "                delta += abs(self.v[state] - new_v)\n",
    "                self.v[state] = new_v\n",
    "                \n",
    "            if delta < 1e-06:\n",
    "                print('策略评估迭代次数:',i)\n",
    "                break\n",
    "\n",
    "    def policy_improve(self):\n",
    "        # 利用更新后的值函数进行策略改善\n",
    "            # 需要对状态空间中的每个状态处的策略进行改善\n",
    "            # 在每个状态处，利用当前的值函数找到对应的使之最大的动作即为贪婪策略取法\n",
    "        yuanyang = self.yuanyang\n",
    "        for state in self.states:\n",
    "            flag1 = 0\n",
    "            flag2 = 0\n",
    "            flag1 = yuanyang.collide(yuanyang.state_to_position(state))\n",
    "            flag2 = yuanyang.find(yuanyang.state_to_position(state))\n",
    "            if flag1 == 1 or flag2 == 1:\n",
    "                continue\n",
    "            \n",
    "            a1 = self.actions[int(random.random()*4)]    # 随机选取一个动作\n",
    "            s, r, t = yuanyang.transform(state,a1)\n",
    "            v1 = r + self.gamma * self.v[s]\n",
    "            \n",
    "            # 对于状态s，采用哪种动作，得到的值函数最大\n",
    "            for action in self.actions:\n",
    "                s, r, t = yuanyang.transform(state, action)\n",
    "                if v1 < r + self.gamma * self.v[s]:\n",
    "                    a1 = action\n",
    "                    v1 = r + self.gamma * self.v[s]\n",
    "            self.pi[state] = a1     # 贪婪策略更新\n",
    "\n",
    "    def policy_iterate(self):\n",
    "        # 循环进行策略评估和策略改善\n",
    "        # 当策略不发生改变时，结束\n",
    "        for i in range(100):\n",
    "            # 策略评估，变的是pi\n",
    "            self.policy_evaluate()\n",
    "            \n",
    "            # 策略改善\n",
    "            pi_old = self.pi.copy()\n",
    "            self.policy_improve()\n",
    "            if (self.pi == pi_old):\n",
    "                print('策略改善次数:',i)\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8be9865b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "策略评估迭代次数: 74\n",
      "策略评估迭代次数: 1\n",
      "策略评估迭代次数: 1\n",
      "策略评估迭代次数: 1\n",
      "策略评估迭代次数: 1\n",
      "策略评估迭代次数: 1\n",
      "策略评估迭代次数: 1\n",
      "策略评估迭代次数: 1\n",
      "策略评估迭代次数: 2\n",
      "策略评估迭代次数: 1\n",
      "策略评估迭代次数: 1\n",
      "策略评估迭代次数: 1\n",
      "策略评估迭代次数: 2\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "策略评估迭代次数: 0\n",
      "0->e\t\n",
      "1->s\t\n",
      "11->e\t\n",
      "12->s\t\n",
      "22->s\t\n",
      "32->s\t\n",
      "42->s\t\n",
      "52->e\t\n",
      "53->e\t\n",
      "54->e\t\n",
      "55->e\t\n",
      "56->e\t\n",
      "57->n\t\n",
      "47->e\t\n",
      "48->e\t\n",
      "49->n\t\n",
      "39->n\t\n",
      "29->n\t\n",
      "19->n\t\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pygame' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 35>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m path\u001b[38;5;241m.\u001b[39mappend(s)\n\u001b[0;32m     34\u001b[0m yuanyang\u001b[38;5;241m.\u001b[39mrender()\n\u001b[1;32m---> 35\u001b[0m \u001b[43mpygame\u001b[49m\u001b[38;5;241m.\u001b[39mquit()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pygame' is not defined"
     ]
    }
   ],
   "source": [
    "yuanyang = YuanYangEnv()\n",
    "policy_value = DP_Policy_Iter(yuanyang)\n",
    "policy_value.policy_iterate()\n",
    "flag = 1\n",
    "s = 0\n",
    "path = []\n",
    "\n",
    "# 将v值打印出来\n",
    "for state in range(100):\n",
    "    i = int(state / 10)\n",
    "    j = state % 10\n",
    "    yuanyang.value[j, i] = policy_value.v[state]\n",
    "step_num = 0\n",
    "\n",
    "# 将最优路径打印出来\n",
    "while flag:\n",
    "    # 渲染路径点\n",
    "    path.append(s)\n",
    "    yuanyang.path = path\n",
    "    a = policy_value.pi[s]\n",
    "    print('%d->%s\\t' % (s, a))\n",
    "    yuanyang.bird_male_position = yuanyang.state_to_position(s)\n",
    "    yuanyang.render()\n",
    "    time.sleep(0.2)\n",
    "    step_num += 1\n",
    "    s_, r, t = yuanyang.transform(s, a)\n",
    "    if t == True or step_num > 200:\n",
    "        flag = 0\n",
    "    s = s_\n",
    "\n",
    "# 渲染最后的路径点\n",
    "yuanyang.bird_male_position = yuanyang.state_to_position(s)\n",
    "path.append(s)\n",
    "yuanyang.render()\n",
    "pygame.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
